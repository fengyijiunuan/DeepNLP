import jieba
import re
import numpy as np
import os
from typing import Dict, Tuple, List


def data_preprocess(root_dir):
    """
    从指定目录读取所有文本文件，清洗文本，只保留中文字符，并去除预设的无关信息。

    参数:
    root_dir (str): 文本文件所在的根目录。

    返回:
    list: 清洗后的文本行组成的列表。
    """
    corpus = []
    # 编译正则表达式，以便重用
    non_chinese_pattern = re.compile(r'[^\u4E00-\u9FA5]')
    # 需要去除的固定文本集合
    irrelevant_texts = {
        "本书来自www.cr173.com免费txt小说下载站",
        "更多更新免费电子书请关注www.cr173.com",
        "新语丝电子文库"
    }

    for file_name in os.listdir(root_dir):
        if file_name.endswith('.txt'):
            path = os.path.join(root_dir, file_name)
            with open(path, 'r', encoding='ANSI') as file:
                lines = [line.strip().replace("\u3000", "").replace("\t", "") for line in file]
                corpus.extend(lines)

    # 清洗每一行文本
    cleaned_corpus = [
        non_chinese_pattern.sub("", line)  # 只保留中文字符
        for line in corpus
        if line not in irrelevant_texts and line.strip() != ""  # 去除无关文本和空行
    ]

    return cleaned_corpus


def char_frequency(file: List[str], n: int) -> Dict[str, int]:
    """
    计算字符频率。

    参数:
    file (List[str]): 文本行的列表。
    n (int): 计算频率的字符长度，1代表单字符，2代表双字符，以此类推。

    返回:
    Dict[str, int]: 字符串及其出现频率的字典。
    """
    adict = {}
    for line in file:
        for i in range(len(line)-(n-1)):
            key = line[i:i+n]
            adict[key] = adict.get(key, 0) + 1
    return adict

def word_frequency(file: List[str], n: int) -> Dict[Tuple[str, ...], int]:
    """
    计算词频。

    参数:
    file (List[str]): 文本行的列表。
    n (int): 计算频率的词长度，1代表单词，2代表双词组合，以此类推。

    返回:
    Dict[Tuple[str, ...], int]: 词组及其出现频率的字典。
    """
    adict = {}
    for line in file:
        words = list(jieba.cut(line))
        for i in range(len(words)-(n-1)):
            key = tuple(words[i:i+n])
            adict[key] = adict.get(key, 0) + 1
    return adict



def cal_cha_entropy(file: List[str], n: int) -> float:
    """
    计算字符熵。

    参数:
    file (List[str]): 文本行的列表。
    n (int): 考虑n元模型。

    返回:
    float: 计算得到的熵值。
    """
    frequency = char_frequency(file, n)
    sums = np.sum(list(frequency.values()))
    if n == 1:
        entropy = -np.sum([i * np.log2(i / sums) for i in frequency.values()]) / sums
    else:
        frequency_n_minus_1 = char_frequency(file, n-1)
        entropy = -np.sum([v * np.log2(v / frequency_n_minus_1[k[:n-1]]) for k, v in frequency.items()]) / sums
    return entropy

def cal_word_entropy(file: List[str], n: int) -> float:
    """
    计算词熵。

    参数:
    file (List[str]): 文本行的列表。
    n (int): 考虑n元模型。

    返回:
    float: 计算得到的熵值。
    """
    frequency = word_frequency(file, n)
    sums = np.sum(list(frequency.values()))
    if n == 1:
        entropy = -np.sum([i * np.log2(i / sums) for i in frequency.values()]) / sums
    else:
        frequency_n_minus_1 = word_frequency(file, n-1)
        entropy = -np.sum([v * np.log2(v / frequency_n_minus_1[tuple(k[:n-1])]) for k, v in frequency.items()]) / sums
    return entropy



root = r'C:\Users\86157\Desktop\jyxstxtqj_downcc.com/'
text_txt = data_preprocess(root)
for n in range(1, 4):
    cha = cal_cha_entropy(text_txt, n)
    word = cal_word_entropy(text_txt, n)
    print(f"基于字的{n}元模型的平均信息熵为: {cha}")
    print(f"基于词的{n}元模型的平均信息熵为: {word}")
